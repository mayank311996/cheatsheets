{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tesla_tweets.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"We are going to implement sentiment analysis with BERT!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sample)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.unk_token, tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.cls_token, tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sep_token, tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-element",
   "metadata": {},
   "source": [
    "Tokenizer.encode_plus adds [CLS] at beginning and [SEP] at end of the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_sample = '[CLS]' + sample + '[SEP]'\n",
    "tokens = tokenizer.tokenize(manual_sample)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(manual_sample)\n",
    "print(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(sample, max_length=24, truncation=True, pad_to_max_length=True,\n",
    "                                add_special_tokens=True, return_attention_mask=True,\n",
    "                                return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-surprise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-notice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-providence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-municipality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-quest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PhraseId', 'SentenceId'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df['Sentiment'].apply(lambda x: 2 if x > 2 else (1 if x == 2 else 0))\n",
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = df['Phrase'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.distplot(seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "SEQLEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(sentence):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        sentence, max_length=SEQLEN, truncation=True,\n",
    "        padding='max_length', add_special_tokens=True,\n",
    "        return_attention_mask=True, return_token_type_ids=False\n",
    "    )\n",
    "    return tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xraw = df['Phrase'].values\n",
    "Xraw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for item in Xraw:\n",
    "    input_i, mask = encoder(item)\n",
    "    input_ids.append(input_id)\n",
    "    attention_mask.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids = np.asarray(input_ids)\n",
    "Xmask = np.asarray(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros((arr.size, arr.max()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.arange(arr.size), arr] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xids.npy', 'wb') as f:\n",
    "    np.save(f, Xids)\n",
    "    \n",
    "with open('xmask.npy', 'wb') as f:\n",
    "    np.save(f, Xmask)\n",
    "    \n",
    "with open('labels.npy', 'wb') as f:\n",
    "    np.save(f, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-format",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-terrorist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-vintage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-problem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel, DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xids.npy', 'rb') as f:\n",
    "    Xids = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xmask.npy', 'rb') as f:\n",
    "    Xmask = np.load(f)\n",
    "with open('labels.npy', 'rb') as f:\n",
    "    labels = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(input_ids, mask, label):\n",
    "    return {'input_ids': input_ids, 'attention_mask': mask}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(map_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(dataset)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.9\n",
    "train = dataset.take(round(size*split))\n",
    "val = dataset.skip(round(size*split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "split*size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "config = DistilBertConfig(num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False  # Setting distilber layer to freeze\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizer.Adam(0.02)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data = val,\n",
    "    epochs = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-inspection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-pickup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "respective-birth",
   "metadata": {},
   "source": [
    "## Building NN models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xids.npy', 'rb') as fp:\n",
    "    Xids = np.load(fp)\n",
    "with open('xmask.npy', 'rb') as fp:\n",
    "    Xmask = np.load(fp)\n",
    "with open('labels.npy', 'rb') as fp:\n",
    "    labels = np.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask':masks}, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(map_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataset = dataset.shuffle(10000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 0.8\n",
    "DS_LEN = len(list(dateset))\n",
    "train = dataset.take(round(DS_LEN*SPLIT))\n",
    "val = dataset.skip(round(DS_LEN*SPLIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = TFAutoModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(50,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(50,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]\n",
    "\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)\n",
    "X = tf.keras.layers.Dense(32, activation='relu')(X)\n",
    "y = tf.keras.layers.Dense(3, activate='softmax', name='outputs')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = False  # This time our BERT is 2nd layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=140\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/nn140')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "mpl.rcParams.update({'font.size': 18})\n",
    "\n",
    "epochs = list(range(len(history.history['accuracy'])))\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.lineplot(x=epochs, y=history.history['accuracy'], label='acc', color='#08c6ab')\n",
    "sns.lineplot(x=epochs, y=history.history['val_accuracy'], label='val-acc', color='#212b38')\n",
    "\n",
    "sns.lineplot(x=epochs, y=history.history['loss'], label='loss', color='#726eff')\n",
    "sns.lineplot(x=epochs, y=history.history['val_loss'], label='val-loss', color='#37465b')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/nn140-metrics.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-times",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-little",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-academy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-corpus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intense-conditions",
   "metadata": {},
   "source": [
    "## Bulding ConveNet Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-spirituality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-sewing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(50,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(50,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]\n",
    "\n",
    "X = tf.keras.layers.Conv1D(filters=50, kernel_size=2, padding='valid', activation = 'relu')(embeddings)\n",
    "X = tf.keras.layers.Conv1D(filters=50, kernel_size=3, padding='valid', activation = 'relu')(X)\n",
    "X = tf.keras.layers.Conv1D(filters=50, kernel_size=4, padding='valid', activation = 'relu')(X)\n",
    "\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)\n",
    "X = tf.keras.layers.Dense(32, activation='relu')(X)\n",
    "y = tf.keras.layers.Dense(3, activate='softmax', name='outputs')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-yemen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-track",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-victorian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "attractive-attribute",
   "metadata": {},
   "source": [
    "## Bulding an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(50,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(50,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]\n",
    "\n",
    "X = tf.keras.layers.LSTM(64)(embeddings)\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)\n",
    "X = tf.keras.layers.Dense(32, activation='relu')(X)\n",
    "y = tf.keras.layers.Dense(3, activate='softmax', name='outputs')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-general",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-title",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-worth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-messaging",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "falling-russia",
   "metadata": {},
   "source": [
    "## Final Model Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.dataset_download_file('kazanova/sentiment140', file_name='training.1600000.processed.noemoticon.csv', path='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('training.1600000.processed.noemoticon.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1',\n",
    "                names=['target', 'id', 'date', 'flag', 'user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df [['target', 'text']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('target')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['target'] == 4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.distplot(seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQLEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.encode_plus(sentence, max_length=SEQLEN,\n",
    "                                  truncation=True, padding='max_length',\n",
    "                                  add_special_tokens=True, return_attention_mask=True,\n",
    "                                  return_token_type_ids=False, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids = np.zeros((len(df), SEQLEN))\n",
    "Xmask = np.zeros((len(df), SEQLEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(df['text']):\n",
    "    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n",
    "    if i % 100000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].apply(lambda x: 1 if x == 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.zeros((arr.size, arr.max()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.arange(arr.size), arr] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter-xids.npy', 'wb') as f:\n",
    "    np.save(f, Xids)\n",
    "with open('twitter-xmask.npy', 'wb') as f:\n",
    "    np.save(f, Xmask)\n",
    "with open('twitter-labels.npy', 'wb') as f:\n",
    "    np.save(f, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, Xids, Xmask, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter-xids.npy', 'rb') as f:\n",
    "    Xids = np.load(f)\n",
    "with open('twitter-xmask.npy', 'rb') as f:\n",
    "    Xmask = np.load(f)\n",
    "with open('twitter-labels.npy', 'rb') as f:\n",
    "    labels = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE = 10000000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask':masks}, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(map_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.shuffle(SHUFFLE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 0.9\n",
    "train = data.take(int(50000*SPLIT))\n",
    "val = data.skip(int(50000*SPLIT))\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = TFAutoModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(SEQLEN,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(SEQLEN,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]\n",
    "\n",
    "x = tf.keras.layers.LSTM(64)(embeddings)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "model.layers[2].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "mpl.rcParams.update({'font.size': 18})\n",
    "\n",
    "epochs = list(range(len(history.history['accuracy'])))\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.lineplot(x=epochs, y=history.history['accuracy'], label='acc', color='#08c6ab')\n",
    "sns.lineplot(x=epochs, y=history.history['val_accuracy'], label='val-acc', color='#212b38')\n",
    "\n",
    "sns.lineplot(x=epochs, y=history.history['loss'], label='loss', color='#726eff')\n",
    "sns.lineplot(x=epochs, y=history.history['val_loss'], label='val-loss', color='#37465b')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/final-acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-payday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-intermediate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-worth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
